{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal as sig\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"/media/newdrive/leto_backup/K6/\")\n",
    "landmark_files = []\n",
    "for subdir in os.listdir(data_root):\n",
    "    for file in os.listdir(data_root/subdir/'Down'):\n",
    "        if re.match(r\"00\\d*DeepCut_resnet50_Down2May25shuffle1_1030000\\.h5\", file):\n",
    "            landmark_files.append(data_root/subdir/'Down'/file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smooth import preproc\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "\n",
    "def read_df(df_file):\n",
    "    df = pd.read_hdf(df_file)\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "    df.index.name = 'index'\n",
    "    df.index = df.index.map(int)\n",
    "    df = df.applymap(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    body_parts = pd.unique([col[0] for col in df.columns])\n",
    "    smoothed_data = {}\n",
    "    for part in body_parts:\n",
    "        smoothed_data[(part, 'x')] = preproc(df[part].x, df[part].likelihood)\n",
    "        smoothed_data[(part, 'y')] = preproc(df[part].y, df[part].likelihood)\n",
    "        smoothed_data[(part, 'likelihood')] = df[part].likelihood.copy()\n",
    "\n",
    "    smooth_df = pd.DataFrame.from_dict(smoothed_data)\n",
    "    return smooth_df\n",
    "\n",
    "\n",
    "def normalize_coordinates(df: pd.DataFrame):\n",
    "    N = len(df)\n",
    "    xy_df = df.drop(axis=1, columns='likelihood', level=1)\n",
    "    coords = xy_df.values.reshape(N, -1, 2)\n",
    "    base_tail_coords = xy_df.tailbase.values[:, np.newaxis, :]\n",
    "    centered_coords = coords - base_tail_coords\n",
    "    centered_nose_coords = xy_df.nose.values - xy_df.tailbase.values\n",
    "    thetas = np.arctan2(centered_nose_coords[:, 1], centered_nose_coords[:, 0])\n",
    "    rotation_matrices = np.stack([np.stack([np.cos(thetas), np.sin(thetas)], axis=-1),\n",
    "                                  np.stack([np.sin(thetas), -np.cos(thetas)], axis=-1)], axis=-1)\n",
    "    normalized_coords = np.einsum(\"nij,nkj->nki\", rotation_matrices, centered_coords)\n",
    "#     print(xy_df.head())\n",
    "#     print((centered_coords[1000]))\n",
    "#     print(normalized_coords[1000])\n",
    "    return normalized_coords\n",
    "\n",
    "\n",
    "class LandmarkDataset(data.Dataset):\n",
    "    def __init__(self, landmarks_file):\n",
    "        super(LandmarkDataset, self).__init__()\n",
    "        self.file = landmarks_file\n",
    "        self.df = read_df(landmarks_file)\n",
    "        self.df = process_df(self.df)\n",
    "        self.coords = normalize_coordinates(self.df)\n",
    "        self.body_parts = pd.unique([col[0] for col in self.df.columns])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.coords[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.coords.shape[0]\n",
    "\n",
    "    \n",
    "class SequenceDataset(Dataset):\n",
    "    eps = 1e-8\n",
    "    def __init__(self, data, seqlen=60, step=10, diff=True, flatten=True):\n",
    "        super(SequenceDataset, self).__init__()\n",
    "        self.seqlen, self.step, self.diff, self.flatten = seqlen, step, diff, flatten\n",
    "        self.data = data\n",
    "        self.mean, self.std = self._mean(), self._std()\n",
    "#         self.mean, self.std = 0, 1\n",
    "\n",
    "    def _mean(self):\n",
    "        if self.diff:\n",
    "            mean = np.zeros_like(self.data[0])\n",
    "        else:\n",
    "            mean = self.data.mean(axis=0)\n",
    "        return mean \n",
    "\n",
    "    def _std(self):\n",
    "        if self.diff:\n",
    "            std = np.diff(self.data, axis=0).std(axis=0)\n",
    "        else:\n",
    "            std = self.data.std(axis=0)\n",
    "        return std\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.seqlen) // self.step\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        offset = self.step * i\n",
    "        item = self.data[offset: offset + self.seqlen]\n",
    "        if self.diff:\n",
    "            item = np.diff(item, axis=0)\n",
    "        item = (item - self.mean) / (self.std + self.eps)\n",
    "        if self.flatten:\n",
    "            item = np.reshape(item, (-1, ))\n",
    "        return item\n",
    "\n",
    "        \n",
    "landmarks_file = Path('/media/newdrive/leto_backup/K6/2020-03-31/Down/0015DeepCut_resnet50_DownMay7shuffle1_1030000.h5')\n",
    "landmarks_data = LandmarkDataset(landmarks_file)\n",
    "data = SequenceDataset(landmarks_data.coords.reshape(len(landmarks_data), -1), seqlen=60, step=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SimpleAutoencoder()\n",
    "m.prepare_data()\n",
    "plt.plot(m.train_ds[0][13::24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_file = landmark_files[2]\n",
    "class SimpleAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, n_neurons=[203, 128, 128, 7], lr=1e-3, seqlen=30):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.seqlen = seqlen\n",
    "        self.hparams = {'lr': lr}\n",
    "        n_layers = len(n_neurons) - 1\n",
    "        layers = list()\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "            if i+1 < n_layers:\n",
    "                layers.append(nn.ELU())\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        layers = list()\n",
    "        n_neurons = n_neurons[::-1]\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "            if i+1 < n_layers:\n",
    "                layers.append(nn.ELU())\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        landmarks_data = LandmarkDataset(landmarks_file)\n",
    "        coords = landmarks_data.coords\n",
    "        coords = sig.decimate(coords, q=4, axis=0)\n",
    "        N, n_coords, _ = coords.shape\n",
    "        train_data = coords[:int(0.8*N)].reshape(-1, n_coords*2).astype(np.float32)\n",
    "        valid_data = coords[int(0.8*N):].reshape(-1, n_coords*2).astype(np.float32)\n",
    "        self.train_ds = SequenceDataset(train_data, seqlen=self.seqlen, step=1, diff=False)\n",
    "        self.valid_ds = SequenceDataset(valid_data, seqlen=self.seqlen, step=10, diff=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # dataset = SequenceDataset(X_val, seqlen=30, step=5, diff=True)\n",
    "        return DataLoader(self.valid_ds, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), self.hparams['lr'])\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.2 ,patience=20, verbose=True, min_lr=1e-6)\n",
    "        return [opt], [sched]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        bx = batch\n",
    "        out = self(bx)\n",
    "        loss = nn.functional.mse_loss(out, bx)\n",
    "        logs = {'loss': loss}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        bx = batch\n",
    "        out = self(bx)\n",
    "        loss = nn.functional.mse_loss(out, bx)\n",
    "        logs = {'loss': loss}\n",
    "        return {'val_loss': loss, 'log': logs}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        losses = torch.stack([out['val_loss'] for out in outputs])\n",
    "#         print(losses.mean())\n",
    "        return {\"val_loss\": losses.mean()}\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleAutoencoder(n_neurons=[2*12*30, 2048, 2048, 1024, 32], lr=1e-4)\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=20, max_epochs=100, )\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = model.val_dataloader()\n",
    "bx = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(bx)\n",
    "plt.plot(bx[12].cpu().numpy().reshape(30, 24)[:,0], label='orig')\n",
    "plt.plot(out[12].cpu().numpy().reshape(30, 24)[:,0], label='recon')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoded_data(data, model, batch_size=256):\n",
    "    dl = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    X = []\n",
    "    model.cuda()\n",
    "    with torch.no_grad():\n",
    "        for bx in dl:\n",
    "            x_encoded = model.encoder(bx.cuda())\n",
    "            X.append(x_encoded.cpu().numpy())\n",
    "    return np.concatenate(X)\n",
    "\n",
    "landmarks_file = Path('/media/newdrive/leto_backup/K6/2020-03-31/Down/0015DeepCut_resnet50_DownMay7shuffle1_1030000.h5')\n",
    "landmarks_file = landmark_files[2]\n",
    "landmarks_data = LandmarkDataset(landmarks_file)\n",
    "coords = sig.decimate(landmarks_data.coords, axis=0, q=4).astype(np.float32)\n",
    "data = SequenceDataset(coords.reshape(len(coords), -1), seqlen=30, step=1, diff=False)\n",
    "\n",
    "X_encoded = create_encoded_data(data, model)\n",
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, confusion_matrix, accuracy_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=30)\n",
    "labels = kmeans.fit_predict(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[90000:110000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[10**5+1500:10**5+3000])\n",
    "# plt.plot(labels[250:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r\"(a+b+c+)+\", \"daaabbbccabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(ord('A'), ord('Z'))] + [chr(i) for i in range(ord('a'), ord('z'))]\n",
    "labels_string = ''.join([chars[l] for l in labels])\n",
    "labels_string[280:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r\"(K+Q+M+d+b+)+\")\n",
    "spans = [match.span() for match in re.finditer(pat, labels_string)]\n",
    "span_lengths = [span[1] - span[0] for span in spans]\n",
    "spans[3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r\"K+(?!Q*K+)\")\n",
    "fspans = [match.span() for match in re.finditer(pat, labels_string)]\n",
    "fspans = [(max(0, s[0]-30), s[1]+30) for s in fspans]\n",
    "fig, axes = plt.subplots(nrows=10, ncols=2, figsize=(18, 20))\n",
    "for i in range(10):    \n",
    "    for ipart, part in enumerate(landmarks_data.body_parts):\n",
    "        if part in ['forepawR', 'forePawL', 'hindpawR', 'hindpawL']:\n",
    "            axes[i][0].plot(coords[fspans[i][0]+15: fspans[i][1]+15,ipart,0], label=f\"{part}_x\")\n",
    "            axes[i][0].plot(coords[fspans[i][0]+15: fspans[i][1]+15,ipart,1], label=f\"{part}_y\")\n",
    "    axes[i][1].plot(labels[slice(*fspans[i])])\n",
    "    axes[i][0].legend(loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(re.findall(r\"K+(?!K*Q+)\", labels_string)))\n",
    "print(len(re.findall(r\"K+Q+(?!Q*M+)\", labels_string)))\n",
    "print(len(re.findall(r\"K+Q+M+(?!M*d+)\", labels_string)))\n",
    "print(len(re.findall(r\"K+Q+M+d+(?!d*b+)\", labels_string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=50, ncols=2, figsize=(18, 200))\n",
    "for i in range(50):    \n",
    "    for ipart, part in enumerate(landmarks_data.body_parts):\n",
    "        if part in ['forepawR', 'forePawL', 'hindpawR', 'hindpawL']:\n",
    "            axes[i][0].plot(coords[spans[i][0]+15: spans[i][1]+15,ipart,0], label=f\"{part}_x\")\n",
    "            axes[i][0].plot(coords[spans[i][0]+15: spans[i][1]+15,ipart,1], label=f\"{part}_y\")\n",
    "    axes[i][1].plot(labels[slice(*spans[i])])\n",
    "    axes[i][0].legend(loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(set(labels))\n",
    "transition_matrix = np.zeros((n_clusters, n_clusters))\n",
    "for i in range(len(labels) - 1):\n",
    "    transition_matrix[labels[i], labels[i+1]] += 1.\n",
    "\n",
    "np.fill_diagonal(transition_matrix, val=0)\n",
    "\n",
    "transition_matrix /= transition_matrix.sum(axis=0, keepdims=True)\n",
    "plt.imshow(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(idx_arr):\n",
    "    to_split = np.where(np.abs(np.diff(idx_arr)) > 1)[0] + 1\n",
    "    return np.split(idx_arr, indices_or_sections=to_split)\n",
    "behaviors = [split(np.where(y_gold==lbl)[0]) for lbl in set(y_gold)]\n",
    "sections = [np.stack([np.mean(X_encoded[sec], axis=0) for sec in beh]) for beh in behaviors]\n",
    "sections[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

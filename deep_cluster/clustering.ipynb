{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import signal as sig\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from torch.utils import data\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataloader import LandmarkDataset, SequenceDataset, LandmarkWaveletDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import normalized_mutual_info_score, confusion_matrix, accuracy_score\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7edd35b070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/Storage1/Data/K7/2020-08-10/Down/model=5_27_10_29_20-video=0041.h5'),\n",
       " PosixPath('/mnt/Storage1/Data/K7/2020-08-04/Down/model=5_27_10_29_20-video=0037.h5'),\n",
       " PosixPath('/mnt/Storage1/Data/K7/2020-08-16/Down/model=5_27_10_29_20-video=0047.h5'),\n",
       " PosixPath('/mnt/Storage1/Data/K7/2020-08-12/Down/model=5_27_10_29_20-video=0044.h5'),\n",
       " PosixPath('/mnt/Storage1/Data/K7/2020-08-13/Down/model=5_27_10_29_20-video=0045.h5'),\n",
       " PosixPath('/mnt/Storage1/Data/K7/2020-08-06/Down/model=5_27_10_29_20-video=0039.h5'),\n",
       " PosixPath('/mnt/Storage1/Data/K7/2020-08-05/Down/model=5_27_10_29_20-video=0038.h5')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root = Path('/mnt/Storage1/Data/K7')\n",
    "\n",
    "# landmark_files = list(data_root.glob('2020-*/Down/*DeepCut*.h5')) # DeepLabCut landmarks\n",
    "landmark_files = list(data_root.glob('2020-*/Down/model=*.h5')) # HourGlass landmarks\n",
    "\n",
    "landmark_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model and dataloader, load saved model\n",
    "\n",
    "from simple_autoencoder import Autoencoder, PLAutoencoder\n",
    "from dataloader import LandmarksDataModule\n",
    "seqlen = 60\n",
    "to_drop = None\n",
    "dm = LandmarksDataModule(landmark_files, seqlen=seqlen, step=1, to_drop=to_drop)\n",
    "dm.prepare_data()\n",
    "n_parts = len(dm.data_frames[0].columns.levels[0])\n",
    "model = PLAutoencoder(landmark_files, n_neurons=[2*n_parts*seqlen, 1024, 512, 512, 10], lr=5e-4, patience=20, dropout=0.1)\n",
    "model_dir = Path('models/HG_landmarks3')\n",
    "model.load_state_dict(torch.load(model_dir / 'model.pt'))"
   ]
  },
  {
   "source": [
    "## Cluster the data\n",
    "\n",
    "cluster the dataset using KMeans on the embedded space.\n",
    " \n",
    " It is possible to filter out timesteps with less than some energy level - to filter out segments with no or little movement, with the parameter filter_by_energy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 961664, 961665, 961666])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates energies of each sliding window in the dataset, can be used for filtering.\n",
    "def calculate_energies(dm):\n",
    "    energies = np.zeros(len(dm.all_ds))\n",
    "    for i, item in enumerate(dm.all_ds):\n",
    "        item = item.reshape((dm.seqlen, -1))\n",
    "        ff, Pxx = sig.periodogram(item.T, fs=dm.fps)\n",
    "        energies[i] = Pxx.T[:10].mean()\n",
    "\n",
    "\n",
    "filter_by_energy = False\n",
    "\n",
    "\n",
    "if filter_by_energy:\n",
    "    X_encoded = model.model.encode(dm.all_ds)\n",
    "    energiess = calcuate_energies(dm)\n",
    "    selected_idxs = np.where(energies > 5e-5)[0]\n",
    "    idx2orig = {idx : i for i, idx in enumerate(selected_idxs)}\n",
    "    K = model.model.encoder[-1].out_features\n",
    "    kmeans = KMeans(K)\n",
    "    labels = kmeans.fit_predict(X_encoded[selected_idxs])\n",
    "    _labels = np.zeros(len(X_encoded), dtype=np.int32) - 2.  # set labels with -2, meaning \"filtered out\"\n",
    "    for i, lbl in enumerate(labels):\n",
    "        _labels[selected_idxs[i]] = lbl # set every non filtered timestep with the appropriate label.\n",
    "    labels = _labels\n",
    "else:\n",
    "    X_encoded = model.model.encode(dm.all_ds)\n",
    "    K = model.model.encoder[-1].out_features\n",
    "    kmeans = KMeans(K)\n",
    "    labels = kmeans.fit_predict(X_encoded)"
   ]
  },
  {
   "source": [
    "## Segment the data into cluster segments\n",
    "\n",
    "Divide the landmarks timeseries to segments such that each segment is a sequence of timesteps with same cluster label.\n",
    "Filter the segments by length, such that segments shorter than 30 timesteps (0.25 seconds) would not be included.\n",
    "\n",
    "Each segment is a triplet of (start_timestep, duration (in timesteps), cluster_label)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "\n",
    "# find the labels for each video file and put in the dictionary\n",
    "# labels_dict = {}\n",
    "# for df in dm.raw_data:\n",
    "#     labels_dict[df.attrs['file']] = np.zeros(len(df), dtype=np.int32) - 1\n",
    "    \n",
    "for df in dm.raw_data:\n",
    "    df['label'] = labels_dict[df.attrs['file']]\n",
    "    \n",
    "segments_dict = {}\n",
    "for file, labels in labels_dict.items():\n",
    "    split_at = np.where(np.diff(labels) != 0)[0] + 1\n",
    "    split_at = np.append(np.zeros(1, dtype=np.int), np.where(np.diff(labels) != 0)[0] + 1)\n",
    "    segments = [(split_at[i-1], split_at[i] - split_at[i-1], labels[split_at[i-1]]) for i in range(1, len(split_at))]\n",
    "    segments = [seg for seg in segments if seg[-1] >= 0]\n",
    "    segments = [seg for seg in segments if seg[1] >= 30]\n",
    "    segments_dict[file] = segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(model_dir / 'kmeans.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans, file)\n",
    "    \n",
    "np.save(model_dir / 'labels.np', labels)\n",
    "\n",
    "with open(model_dir / 'labels_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(labels_dict, file)\n",
    "    \n",
    "with open(model_dir / 'data_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(data_dict, file)\n",
    "    \n",
    "with open(model_dir / 'segments_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(segment_dict, file)\n",
    "    \n",
    "with open(model_dir / 'x_encoded_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(X_encoded_dict, file)"
   ]
  },
  {
   "source": [
    "## Save clips of clusters\n",
    "\n",
    "for each cluster, sample some segments from the cluster and save them as clips in a folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import tkinter as tk\n",
    "from triplets import landmarks_video, triplets_gui\n",
    "\n",
    "cluster_dir = Path(\"/mnt/Storage1/shuki/projects/clusters/HG_landmarks_clusters\")\n",
    "\n",
    "# write the clip to a video file\n",
    "def write_video(vid, file, fps, codec='mp4v'):\n",
    "    n_frames, width, height, _ = vid.shape\n",
    "    fourcc = cv.VideoWriter_fourcc(*codec)\n",
    "    writer = cv.VideoWriter(str(file), fourcc, fps, (height, width), True)\n",
    "    for frame in vid:\n",
    "        writer.write(frame)\n",
    "    writer.release()\n",
    "\n",
    "frame_to_time = lambda idx: f'{idx // (video.fps*60)}_{(idx % (video.fps*60)) // video.fps}'\n",
    "\n",
    "os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "for file in dm.landmark_files:\n",
    "    video = landmarks_video.LandmarksVideo(file.parent)\n",
    "    file_id = file.name[:4]\n",
    "    segments = segments_dict[file]\n",
    "    for start_frame, n_frames, lbl in segments:\n",
    "        mid_frame = start_frame + n_frames // 2\n",
    "        n_frames = min(120 * 2, n_frames) # max 240 frames in a clip\n",
    "        start, end = mid_frame - n_frames // 2 , mid_frame + n_frames // 2\n",
    "        save_dir = cluster_dir / f'{lbl}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        clip = video[start: end: 2] # \n",
    "        video_file_name = f'{file_id}_{frame_to_time(mid_frame)}.avi'\n",
    "        write_video(clip, save_dir / video_file_name, fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('deep_cluster': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "dde1dfa77f0fd8b2ebe61ad1a947a7c9d16427c1b7c7e095606c7863afad4f80"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
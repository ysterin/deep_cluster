{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import signal as sig\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from torch.utils import data\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataloader import LandmarkDataset, SequenceDataset, LandmarkWaveletDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import normalized_mutual_info_score, confusion_matrix, accuracy_score\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory of data\n",
    "data_root = Path(\"/home/orel/Storage/Data/K6/\")\n",
    "landmark_files = []\n",
    "for subdir in os.listdir(data_root):\n",
    "    for file in os.listdir(data_root/subdir/'Down'):\n",
    "        if re.match(r\"00\\d*DeepCut_resnet50_Down2May25shuffle1_1030000\\.h5\", file):\n",
    "            lfile = data_root/subdir/'Down'/file\n",
    "            landmark_files.append(lfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on\n",
    "from simple_autoencoder import Autoencoder, PLWaveletAutoencoder\n",
    "model = PLWaveletAutoencoder(landmark_files[:5], n_neurons=[480, 512, 512, 30], lr=1e-3, patience=20)\n",
    "# model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx = next(iter(model.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=10, max_epochs=50, logger=pl.loggers.WandbLogger(\"wavelet landmarks autoencoder\"))\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 30\n",
    "X_encoded = model.model.encode(model.all_ds)\n",
    "kmeans = KMeans(K)\n",
    "labels = kmeans.fit_predict(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(labels).most_common(), len(set(labels)), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(labels).most_common(), len(set(labels)), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(labels).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(labels):\n",
    "    split_at = np.where(np.diff(labels) != 0)[0] + 1\n",
    "    sequence = [[seg[0], split_at[i-1]*4, len(seg)*4] \\\n",
    "                for i, seg in enumerate(np.split(labels, indices_or_sections=split_at))]\n",
    "    sequence[0][1] = 0\n",
    "    return sequence\n",
    "\n",
    "segments = split_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = set([s[0] for s in segments])\n",
    "segment_lengths_by_cluster = {c: [seg[2] for seg in segments if seg[0] == c] for c in clusters}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 5; nrows = 6\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(24, 24))\n",
    "bins=np.log(np.linspace(1, 1000, 100))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        cluster_id = i*ncols + j\n",
    "        axes[i][j].set_title(f\"cluster {cluster_id}, with {len(segment_lengths_by_cluster[cluster_id])} segments\")\n",
    "        axes[i][j].hist(segment_lengths_by_cluster[cluster_id], bins=100, log=False, density=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 5; nrows = 6\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(24, 24))\n",
    "bins=np.log(np.linspace(1, 1000, 100))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        cluster_id = i*ncols + j\n",
    "        axes[i][j].set_title(f\"cluster {cluster_id}, with {len(segment_lengths_by_cluster[cluster_id])} segments\")\n",
    "        axes[i][j].hist(np.log(segment_lengths_by_cluster[cluster_id]), bins=100, log=True, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(labels):\n",
    "    split_at = np.where(np.diff(labels) != 0)[0] + 1\n",
    "    sequence = [[seg[0], split_at[i-1]*4 + model.seqlen*2, len(seg)*4] \\\n",
    "                for i, seg in enumerate(np.split(labels, indices_or_sections=split_at))]\n",
    "    sequence[0][1] = model.seqlen*2\n",
    "    return sequence\n",
    "\n",
    "labels_dict = dict(zip(landmark_files, \n",
    "                        np.split(labels, indices_or_sections=video_change_idxs)))\n",
    "\n",
    "data_dict = dict(zip(landmark_files,\n",
    "                    np.split(all_data, indices_or_sections=video_change_idxs)))\n",
    "\n",
    "X_encoded_dict = dict(zip(landmark_files,\n",
    "                         np.split(X_encoded, indices_or_sections=video_change_idxs)))\n",
    "\n",
    "\n",
    "segment_dict = dict(zip(landmark_files, \n",
    "                        map(split_labels, np.split(labels, indices_or_sections=video_change_idxs))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
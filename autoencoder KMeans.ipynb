{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal as sig\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smooth import preproc\n",
    "from pathlib import Path\n",
    "from dataloader import LandmarkDataset, SequenceDataset\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"/media/newdrive/leto_backup/K6/\")\n",
    "landmark_files = []\n",
    "for subdir in os.listdir(data_root):\n",
    "    for file in os.listdir(data_root/subdir/'Down'):\n",
    "        if re.match(r\"00\\d*DeepCut_resnet50_Down2May25shuffle1_1030000\\.h5\", file):\n",
    "            lfile = data_root/subdir/'Down'/file\n",
    "            landmark_files.append(lfile)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "landmarks_file = Path('/media/newdrive/leto_backup/K6/2020-03-31/Down/0015DeepCut_resnet50_DownMay7shuffle1_1030000.h5')\n",
    "landmarks_data = LandmarkDataset(landmarks_file)\n",
    "data = SequenceDataset(landmarks_data.coords.reshape(len(landmarks_data), -1), seqlen=60, step=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_file = landmark_files[2]\n",
    "class SimpleAutoencoder(pl.LightningModule):\n",
    "    def __init__(self, n_neurons=[203, 128, 128, 7], lr=1e-3, seqlen=30, landmark_files=landmark_files):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.landmark_files = landmark_files\n",
    "        self.seqlen = seqlen\n",
    "        self.hparams = {'lr': lr}\n",
    "        n_layers = len(n_neurons) - 1\n",
    "        layers = list()\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "            if i+1 < n_layers:\n",
    "                layers.append(nn.ELU())\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        layers = list()\n",
    "        n_neurons = n_neurons[::-1]\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_neurons[i], n_neurons[i+1]))\n",
    "            if i+1 < n_layers:\n",
    "                layers.append(nn.ELU())\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        landmark_datasets = []\n",
    "        for file in self.landmark_files:\n",
    "            try:\n",
    "                ds = LandmarkDataset(file)\n",
    "                landmark_datasets.append(ds)\n",
    "            except OSError:\n",
    "                pass\n",
    "        coords = [sig.decimate(ds.coords, q=4, axis=0).astype(np.float32) for ds in landmark_datasets]\n",
    "        N, n_coords, _ = coords[0].shape\n",
    "        train_data = [crds[:int(0.8*crds.shape[0])].reshape(-1, n_coords*2) for crds in coords]\n",
    "        valid_data = [crds[int(0.8*crds.shape[0]):].reshape(-1, n_coords*2) for crds in coords]\n",
    "        train_dsets = [SequenceDataset(data, seqlen=self.seqlen, step=1, diff=False) for data in train_data]\n",
    "        valid_dsets = [SequenceDataset(data, seqlen=self.seqlen, step=10, diff=False) for data in valid_data]\n",
    "        self.train_ds = ConcatDataset(train_dsets)\n",
    "        self.valid_ds = ConcatDataset(valid_dsets)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # dataset = SequenceDataset(X_val, seqlen=30, step=5, diff=True)\n",
    "        return DataLoader(self.valid_ds, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), self.hparams['lr'])\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.2 ,patience=20, verbose=True, min_lr=1e-6)\n",
    "        return [opt], [sched]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        bx = batch\n",
    "        out = self(bx)\n",
    "        loss = nn.functional.mse_loss(out, bx)\n",
    "        logs = {'loss': loss}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        bx = batch\n",
    "        out = self(bx)\n",
    "        loss = nn.functional.mse_loss(out, bx)\n",
    "        logs = {'loss': loss}\n",
    "        return {'val_loss': loss, 'log': logs}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        losses = torch.stack([out['val_loss'] for out in outputs])\n",
    "#         print(losses.mean())\n",
    "        return {\"val_loss\": losses.mean()}\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleAutoencoder(n_neurons=[2*12*30, 2048, 1024, 512, 32], lr=1e-4)\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=12, max_epochs=50, )\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = model.val_dataloader()\n",
    "bx = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(bx)\n",
    "plt.plot(bx[150].cpu().numpy().reshape(30, 24)[:,0], label='orig')\n",
    "plt.plot(out[150].cpu().numpy().reshape(30, 24)[:,0], label='recon')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoded_data(data, model, batch_size=256):\n",
    "    dl = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    X = []\n",
    "    model.cuda()\n",
    "    with torch.no_grad():\n",
    "        for bx in dl:\n",
    "            x_encoded = model.encoder(bx.cuda())\n",
    "            X.append(x_encoded.cpu().numpy())\n",
    "    return np.concatenate(X)\n",
    "\n",
    "landmarks_file = landmark_files[2]\n",
    "landmarks_data = LandmarkDataset(landmarks_file)\n",
    "coords = sig.decimate(landmarks_data.coords, axis=0, q=4).astype(np.float32)\n",
    "data = SequenceDataset(coords.reshape(len(coords), -1), seqlen=30, step=1, diff=False)\n",
    "\n",
    "X_encoded = create_encoded_data(data, model)\n",
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, confusion_matrix, accuracy_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=30)\n",
    "labels = kmeans.fit_predict(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "split_at = np.where(np.diff(labels) != 0)[0] + 1\n",
    "sequence = [[s[0], split_at[i-1], len(s)] for i, s in enumerate(np.split(labels, indices_or_sections=split_at))]\n",
    "sequence[0][1] = 0\n",
    "seg_lengths = defaultdict(list)\n",
    "for seg in sequence:\n",
    "    seg_lengths[seg[0]].append(seg[2])\n",
    "\n",
    "sequence = [seq for seq in sequence if seq[2] > 5]\n",
    "\n",
    "cluster_frames = defaultdict(list)\n",
    "for seq in sequence:\n",
    "    cluster_frames[seq[0]].append((seq[1]*4 + seq[2]*2, seq[2]*2))\n",
    "\n",
    "cluster_frames = {c:cl for c, cl in cluster_frames.items() if len(cl)>25}\n",
    "# len(cluster_frames)\n",
    "cluster_samples = {c: random.choices(cl, k=15) for c, cl in cluster_frames.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softplus(torch.Tensor(np.array([-3])), beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import importlib\n",
    "import clip_videos\n",
    "importlib.reload(clip_videos)\n",
    "\n",
    "video_file = data_root/'2020-03-23'/'Down'/'0008DeepCut_resnet50_Down2May25shuffle1_1030000_labeled.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl_id,  cluster in cluster_samples.items():\n",
    "    n_frames = [c[1] for c in cluster]\n",
    "    print(cl_id, np.mean(n_frames), np.std(n_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = LandmarkDataset(landmarks_file).df\n",
    "for cl_id,  cluster in cluster_samples.items():\n",
    "    mid_frames = [c[0] for c in cluster]\n",
    "    n_frames = [c[1] for c in cluster]\n",
    "    n_frames = int(min(60, np.mean(n_frames) + np.std(n_frames)))\n",
    "    print(mid_frames)\n",
    "    clip_videos.save_collage_with_labels_short(str(video_file), df, mid_frames, n_frames_around=n_frames,\n",
    "                                         save_file=f'clusters/example_1/cluster_{cl_id}.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.write(np.zeros())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls clusters/example_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = [seq[0] for seq in sequence]\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def count_ngrams(sequence, max_n=10):\n",
    "    N = len(sequence)\n",
    "    counter = defaultdict(int)\n",
    "    for k in range(1, max_n):\n",
    "        for i in range(N-k):\n",
    "            counter[tuple(sequence[i:i+k])] += 1\n",
    "    return counter\n",
    "\n",
    "def segment_sequence(sequence, max_n=5):\n",
    "    N = len(sequence)\n",
    "    sequence = tuple(sequence)\n",
    "    ngram_count = count_ngrams(sequence, max_n=max_n)\n",
    "    u_arr = np.zeros(N - 1)\n",
    "    for k in range(0, N - 1):\n",
    "        u_k = 0\n",
    "        for n in range(2, min(max_n, k, N-k)):\n",
    "            s_1, s_2 = ngram_count[sequence[k-n+1:k+1]], ngram_count[sequence[k+1:k+n+1]]\n",
    "            u_k += np.mean([1 if s_1 >= ngram_count[sequence[k-n+i+1:k+i+1]] else 0 for i in range(1, n)])\n",
    "            u_k += np.mean([1 if s_2 >= ngram_count[sequence[k-n+i+1:k+i+1]] else 0 for i in range(1, n)])\n",
    "        u_arr[k] = u_k / max_n / 2\n",
    "    \n",
    "    sequence = list(sequence)\n",
    "    segments = []\n",
    "    prev_idx = 0\n",
    "    for idx in range(1, N-1):\n",
    "        if u_arr[idx-1] < u_arr[idx] and u_arr[idx+1] < u_arr[idx]:\n",
    "            segments.append(sequence[prev_idx:idx+1])\n",
    "            prev_idx = idx+1\n",
    "\n",
    "    segments.append(sequence[prev_idx:])\n",
    "    return segments\n",
    "\n",
    "segments = segment_sequence(sequence, max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(map(tuple, segments)).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:100]\n",
    "27, 8, 26, 4, 10, 27, 8, 26, 4, 10, 27, 8, 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[3*10**4+1500:3*10**4+3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[10**5+1500:10**5+3000])\n",
    "# plt.plot(labels[250:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r\"(a+b+c+)+\", \"daaabbbccabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(ord('A'), ord('Z'))] + [chr(i) for i in range(ord('a'), ord('z'))]\n",
    "labels_string = ''.join([chars[l] for l in labels])\n",
    "labels_string[280:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r\"(K+Q+M+d+b+)+\")\n",
    "spans = [match.span() for match in re.finditer(pat, labels_string)]\n",
    "span_lengths = [span[1] - span[0] for span in spans]\n",
    "spans[3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r\"K+(?!Q*K+)\")\n",
    "fspans = [match.span() for match in re.finditer(pat, labels_string)]\n",
    "fspans = [(max(0, s[0]-30), s[1]+30) for s in fspans]\n",
    "fig, axes = plt.subplots(nrows=10, ncols=2, figsize=(18, 20))\n",
    "for i in range(10):    \n",
    "    for ipart, part in enumerate(landmarks_data.body_parts):\n",
    "        if part in ['forepawR', 'forePawL', 'hindpawR', 'hindpawL']:\n",
    "            axes[i][0].plot(coords[fspans[i][0]+15: fspans[i][1]+15,ipart,0], label=f\"{part}_x\")\n",
    "            axes[i][0].plot(coords[fspans[i][0]+15: fspans[i][1]+15,ipart,1], label=f\"{part}_y\")\n",
    "    axes[i][1].plot(labels[slice(*fspans[i])])\n",
    "    axes[i][0].legend(loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(re.findall(r\"K+(?!K*Q+)\", labels_string)))\n",
    "print(len(re.findall(r\"K+Q+(?!Q*M+)\", labels_string)))\n",
    "print(len(re.findall(r\"K+Q+M+(?!M*d+)\", labels_string)))\n",
    "print(len(re.findall(r\"K+Q+M+d+(?!d*b+)\", labels_string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=50, ncols=2, figsize=(18, 200))\n",
    "for i in range(50):    \n",
    "    for ipart, part in enumerate(landmarks_data.body_parts):\n",
    "        if part in ['forepawR', 'forePawL', 'hindpawR', 'hindpawL']:\n",
    "            axes[i][0].plot(coords[spans[i][0]+15: spans[i][1]+15,ipart,0], label=f\"{part}_x\")\n",
    "            axes[i][0].plot(coords[spans[i][0]+15: spans[i][1]+15,ipart,1], label=f\"{part}_y\")\n",
    "    axes[i][1].plot(labels[slice(*spans[i])])\n",
    "    axes[i][0].legend(loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(set(labels))\n",
    "transition_matrix = np.zeros((n_clusters, n_clusters))\n",
    "for i in range(len(labels) - 1):\n",
    "    transition_matrix[labels[i], labels[i+1]] += 1.\n",
    "\n",
    "np.fill_diagonal(transition_matrix, val=0)\n",
    "\n",
    "transition_matrix /= transition_matrix.sum(axis=0, keepdims=True)\n",
    "plt.imshow(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(idx_arr):\n",
    "    to_split = np.where(np.abs(np.diff(idx_arr)) > 1)[0] + 1\n",
    "    return np.split(idx_arr, indices_or_sections=to_split)\n",
    "behaviors = [split(np.where(y_gold==lbl)[0]) for lbl in set(y_gold)]\n",
    "sections = [np.stack([np.mean(X_encoded[sec], axis=0) for sec in beh]) for beh in behaviors]\n",
    "sections[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
